---
title: "Milestone Report"
author: "Christopher Stewart"
date: "March 28, 2015"
output: html_document
---

# Introduction

This document reports on progress towards a text prediction algorithm and Shiny application undertaken in the Capstone project marking the end of the 9-course [Data Science Specialization offered through Coursera](https://www.coursera.org/specialization/jhudatascience/1) and the [John Hopkins Department of Biostatistics](http://www.jhsph.edu/departments/biostatistics/). The project is offered in cooperation with [Swiftkey](http://swiftkey.com/en/), a company building smart prediction technology for easier mobile typing.  

This particular presentation is available as an [R Studio Presenter presentation](http://rpubs.com/cmstewart/milestone_report_final).

## Downloading and Inspecting the Data

The data is provided for us, so the first step is to download it, then get an idea of the size of the files' sizes. We know _a priori_ that the data set consists of 3 large text corpora taken from blogs, news and Twitter. *We report on the number of words in a subsample of the data after tokenization (_see below_).*

```{r}
suppressPackageStartupMessages(require("downloader")); suppressPackageStartupMessages(require("R.utils"))

# Download, unzip data and setwd()
url <- "http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download(url, dest = "data.zip", mode = "wb")
unzip("data.zip", exdir = "./")

# Set working directory
setwd(paste(getwd(),"/final/en_US",sep=""))
list.files()

# Get an idea of corpora sizes
as.numeric(countLines("en_US.blogs.txt"))
as.numeric(countLines("en_US.news.txt"))
as.numeric(countLines("en_US.twitter.txt"))

# Read in data
blogs <- readLines("en_US.blogs.txt"); news <- readLines("en_US.news.txt"); tweets <- readLines("en_US.twitter.txt", skipNul = TRUE)

# Clean up
rm(url)
```


# Sampling 

Seeing the size of the data set, we elect to take a random sample of a size sufficient to guarantee representativeness. The following code chunk performs this sampling, reporting only the size of the sample drawn from the three corpora.

```{r echo = FALSE}
suppressPackageStartupMessages(require("stats"))

# Compute sample size needed
sample_size <- round(power.t.test(n = NULL, sig.level = .01, power = .8, d = .05)$n, digits = 0)

# Make samples
blogs_samp <- sample(blogs, sample_size)
length(blogs_samp)
news_samp <- sample(news, sample_size)
length(news_samp)
tweets_samp <- sample(tweets, sample_size)
length(tweets_samp)

# Clean up
rm(sample_size); rm(blogs); rm(news); rm(tweets); rm(news_samp); rm(tweets_samp)

```


# Major Features of the Data 

With a more manageable sampled subset of the data, we briefly look at some of the more interesting / challenging features of the data for text prediction. In order to get an idea of such features, we  look at the last 5 lines in the sample drawn from the blogs sample.

```{r looking at features of the data}
tail(blogs_samp, n = 5)
```


For the purposes of text prediction, these lines reveal several types of characters that should be eliminated as they shouldn't be predicted, including punctuation, quotation marks and digits. Furthermore, this and the three other samples are likely to contain non-English characters, emoticons, curse words and other things we wouldn't like to predict. As such, we clean the samples.  The following code chunk performs this data cleaning with the **blogs** corpus sample _only_, displaying the first 5 lines of the _cleaned_ blog sample. 


```{r echo = FALSE}
suppressPackageStartupMessages(require("stringr"))

profanity_list <- readLines("http://www.bannedwordlist.com/lists/swearWords.txt", warn = FALSE)

## Blogs
blogs_samp_1 <- tolower(blogs_samp)
blogs_samp_2 <- str_replace_all(blogs_samp_1, "[^[:alnum:]]", " "); blogs_samp_2 <- iconv(blogs_samp_2, from="UTF-8", to="ascii", sub=""); blogs_samp_2 <- iconv(blogs_samp_2, to="ASCII//TRANSLIT")
blogs_samp_3 <- str_replace_all(blogs_samp_2, "[[:digit:]]+", " ")
blogs_samp_4 <- str_replace_all(blogs_samp_3, paste(profanity_list, collapse = "|"), replacement = "")
blogs_samp_5 <- str_replace_all(blogs_samp_4, "  ", replacement = " ")
head(blogs_samp_5, n = 5)
write.table(blogs_samp_5, file = "blogs_sample.txt")

rm(profanity_list)
rm(blogs_samp); rm(blogs_samp_1); rm(blogs_samp_2); rm(blogs_samp_3); rm(blogs_samp_4); rm(blogs_samp_5)

```


# Tokenization, n-gram construction and initial visual data exploration

The next step is to tokenize and build n-grams, which we will then use to predict the posterior probability of a word given the previous context. We have chosen to use R's [stylo](https://sites.google.com/site/computationalstylistics/stylo) package to tokenize and produce n-grams. The following code does so for the _blogs_ sample, reporting the **total number of words** in this sample.The code chunk then displays the 20 most common 1-, 2-, 3- and 4-grams in that sample.


```{r echo = FALSE}
suppressPackageStartupMessages(require("stylo")); suppressPackageStartupMessages(require("ggplot2")); suppressPackageStartupMessages(require("gridExtra"))

# Build corpus, tokenize and produce n-grams
blogs_corp <- load.corpus(files = "blogs_sample.txt")
blogs_tok <- txt.to.words(blogs_corp)

blogs_tok_2 <- make.ngrams(blogs_tok, ngram.size = 2); blogs_tok_3 <- make.ngrams(blogs_tok, ngram.size = 3); blogs_tok_4 <- make.ngrams(blogs_tok, ngram.size = 4)

# Build word lists for n-grams
blogs_words = names(sort(table(unlist(blogs_tok)), decreasing = TRUE)); blogs_words_2 = names(sort(table(unlist(blogs_tok_2)), decreasing = TRUE)); blogs_words_3 = names(sort(table(unlist(blogs_tok_3)), decreasing = TRUE)); blogs_words_4 = names(sort(table(unlist(blogs_tok_4)), decreasing = TRUE)) 

print(length(blogs_words))
      

# Build frequency counts and tables
x_name <- "grams"; y_name <- "frequency"

blogs_freq_1 <- as.numeric(make.table.of.frequencies(blogs_tok, blogs_words, relative = FALSE))
blogs_freq_tab_1 <- data.frame(blogs_words, blogs_freq_1); names(blogs_freq_tab_1) <- c(x_name, y_name)

blogs_freq_2 <- as.numeric(make.table.of.frequencies(blogs_tok_2, blogs_words_2, relative = FALSE))
blogs_freq_tab_2 <- data.frame(blogs_words_2, blogs_freq_2); names(blogs_freq_tab_2) <- c(x_name, y_name)

blogs_freq_3 <- as.numeric(make.table.of.frequencies(blogs_tok_3, blogs_words_3, relative = FALSE))
blogs_freq_tab_3 <- data.frame(blogs_words_3, blogs_freq_3); names(blogs_freq_tab_3) <- c(x_name, y_name)

blogs_freq_4 <- as.numeric(make.table.of.frequencies(blogs_tok_4, blogs_words_4, relative = FALSE))
blogs_freq_tab_4 <- data.frame(blogs_words_4, blogs_freq_4); names(blogs_freq_tab_4) <- c(x_name, y_name)


# Build barplots of frequent n-grams in blogs corpus sample

## Blogs 1-grams
blogs_freq_tab_1_sub <- as.data.frame(head(blogs_freq_tab_1, n = 20)); blogs_freq_tab_1_sub$order <- factor(blogs_freq_tab_1_sub$gram, as.character(blogs_freq_tab_1_sub$gram))

blogs_freq_tab_1_sub.plot <- ggplot(blogs_freq_tab_1_sub, aes(x = blogs_freq_tab_1_sub$order, y = blogs_freq_tab_1_sub$frequency)) + 
    geom_bar(stat="identity") + coord_flip() + xlab("") + ylab("") + ggtitle("Top 20 1-grams in Blogs Corpus") +
    geom_bar(colour="green", fill="blue", width=.7, stat="identity") + 
    theme(plot.background = element_blank() ,panel.grid.major = element_blank() ,panel.grid.minor = element_blank(),     
    panel.border = element_blank())

## Blogs 2-grams
blogs_freq_tab_2_sub <- as.data.frame(head(blogs_freq_tab_2, n = 20)); blogs_freq_tab_2_sub$order <- factor(blogs_freq_tab_2_sub$gram, as.character(blogs_freq_tab_2_sub$gram))

blogs_freq_tab_2_sub.plot <- ggplot(blogs_freq_tab_2_sub, aes(x = blogs_freq_tab_2_sub$order, y = blogs_freq_tab_2_sub$frequency)) + 
    geom_bar(stat="identity") + coord_flip() + xlab("") + ylab("") + ggtitle("Top 20 2-grams in blogs Corpus") +
    geom_bar(colour="pink", fill="purple", width=.7, stat="identity") + 
    theme(plot.background = element_blank() ,panel.grid.major = element_blank() ,panel.grid.minor = element_blank(),     
    panel.border = element_blank())

## Blogs 3-grams

blogs_freq_tab_3_sub <- as.data.frame(head(blogs_freq_tab_3, n = 20)); blogs_freq_tab_3_sub$order <- factor(blogs_freq_tab_3_sub$gram, as.character(blogs_freq_tab_3_sub$gram))

blogs_freq_tab_3_sub.plot <- ggplot(blogs_freq_tab_3_sub, aes(x = blogs_freq_tab_3_sub$order, y = blogs_freq_tab_3_sub$frequency)) + 
    geom_bar(stat="identity") + coord_flip() + xlab("") + ylab("") + ggtitle("Top 20 3-grams in Twitter Corpus") +
    geom_bar(colour="green", fill="yellow", width=.7, stat="identity") + 
    theme(plot.background = element_blank() ,panel.grid.major = element_blank() ,panel.grid.minor = element_blank(),     
    panel.border = element_blank())

## Blogs 4-grams

blogs_freq_tab_4_sub <- as.data.frame(head(blogs_freq_tab_4, n = 20)); blogs_freq_tab_4_sub$order <- factor(blogs_freq_tab_4_sub$gram, as.character(blogs_freq_tab_4_sub$gram))

blogs_freq_tab_4_sub.plot <- ggplot(blogs_freq_tab_4_sub, aes(x = blogs_freq_tab_4_sub$order, y = blogs_freq_tab_4_sub$frequency)) + 
    geom_bar(stat="identity") + coord_flip() + xlab("") + ylab("") + ggtitle("Top 20 4-grams in Blogs Corpus") +
    geom_bar(colour="orange", fill="turquoise", width=.7, stat="identity") + 
    theme(plot.background = element_blank() ,panel.grid.major = element_blank() ,panel.grid.minor = element_blank(),     
    panel.border = element_blank())


## Combine all barplots
newplot <- grid.arrange(blogs_freq_tab_1_sub.plot, blogs_freq_tab_2_sub.plot, blogs_freq_tab_3_sub.plot, blogs_freq_tab_4_sub.plot, ncol = 2)


## Clean up
rm(blogs_corp); rm(x_name); rm(y_name)
rm(blogs_tok); rm(blogs_tok_2); rm(blogs_tok_3); rm(blogs_tok_4)
rm(blogs_words); rm(blogs_words_2); rm(blogs_words_3); rm(blogs_words_4)
rm(blogs_freq_1); rm(blogs_freq_2); rm(blogs_freq_3); rm(blogs_freq_4)
  
```


# Next steps

## Remaining Issues

This report brings up a few potential thorny issues that have yet to be addressed. Chief among these is that of contractions. As the barplots reveal, contractions are currently in the data as two words (e.g "don't" is "don" + "t"). Experimentation with the text prediction models should show if this is the correct choice or not.

## Text Prediction Algorithm and Shiny Data Application 

Now that we have a sorted list of n-grams, the next steps are to write a function that accepts a string of words and attempts to predict the next word. We anticipate doing so by searching our sorted lists for the largest matching string, then taking the word that most often occurs next. For example, if the user types "I am going", the function finds this trigram in the data and returns the word from the 4-gram table that most often completes that sequence in the observed data. If "I am going to" occurs, for example, 76 times and "I am going for" occurs 52 times, the function would return "to" as the predicted word.

If the string *does not* occur in the data, we plan to revert to a smaller n-gram. Such a "backoff" mechanism would complete a progressively smaller sequence so that if the user enters "Colorless clouds float" and there are no occurrences in the data, the function looks for "clouds float" and eventually even "float", returning the most probable next word in each case. 

Once the function is written, we will begin training a model using increasingly higher sample sizes, until computing capacity is reached. At this point, we will explore avenues for making the model more efficient via more efficient coding, better data structures, etc. We then pla to build these structures into a Shiny application, remaining as faithful as possible to the structure and experience described herein.


# Thank you very much for your time. We would greatly appreciate any feedback you might care to provide!