---
title: "Capstone Project"
author: "Christopher Stewart"
date: "March 25, 2015"
output: html_document
---
## Introduction 
This document reports on the Capstone project marking the end of the 9-course Data Science Specialization offered by Coursera and the John Hopkins Department of Biostatistics. The purpose of this project is to apply the knowledge gained throughout the specialization's courses to a novel data science problem: text prediction. Specifically, we use corpora to build a text prediction algorithmthat is then incorporated into an interface that can be accessed by others. The project is offered in cooperation with Swiftkey, a company building smart prediction technology for easier mobile typing. Documentation on my Shiny data product is available in an [R Studio Presenter presentation] (insert URL here).

I have elected to complete the project in R as per the parameters of the assignment, but also in Python to get hands-on experience with the Python Natural Language Toolkit, or [NLTK] (http://www.nltk.org/). A report on the Python version of the project is available [here] (insert URL here).

## Data preparation and exploration

### Data Acquisition

Initially, we download the data and unzip the data files, switching working directories to that of the English language text files. We then print out the contents of the current directory and inspect the number of lines in each of the corpora.

```{r}
require("downloader"); require("R.utils")

# Download, unzip data and setwd()
url <- "http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download(url, dest = "data.zip", mode = "wb")
unzip("data.zip", exdir = "./")

# Set working directory
setwd(paste(getwd(),"/final/en_US",sep=""))
list.files()

# Get an idea of corpora sizes in lines
as.numeric(countLines("en_US.blogs.txt"))
as.numeric(countLines("en_US.news.txt"))
as.numeric(countLines("en_US.twitter.txt"))

# Clean up
rm(url)
```


### Data Sampling

Seeing the large size of the corpora and cognizant of the fact that a representative sample is sufficient for model building, we next take a random subsample of the data. We first compute the n necessary for a representative sample size, then we draw a random sample from each of the corpora. 

```{r}
require("stats")
dir.create(path = "./samples/")

# Compute sample size needed
sample_size <- round(power.t.test(n = NULL, sig.level = .01, power = .8, d = .05)$n, digits = 0)
# n = 9345

# Read in data
blogs <- readLines("en_US.blogs.txt"); news <- readLines("en_US.news.txt"); tweets <- readLines("en_US.twitter.txt", skipNul = TRUE)

# Get an idea of approximate word counts
blogs_word_count <- length(strsplit(blogs, " "))
print(blogs_word_count)
news_word_count <- length(strsplit(news, " "))
print(news_word_count)
tweets_word_count <- length(strsplit(tweets, " "))
print(tweets_word_count)

# Make samples
blogs_samp <- sample(blogs, sample_size)
news_samp <- sample(news, sample_size)
tweets_samp <- sample(tweets, sample_size)

# Clean up
rm(sample_size); rm(blogs); rm(news); rm(tweets)
rm(blogs_word_count); rm(news_word_count); rm(tweets_word_count)
```


### Sample Cleaning and Profanity Removal

For corpus creation, we first clean up our corpora by removing upper-case letters and non-alphanumeric characters. Next, we extract profanity. Finally, we write the resulting samples out to text files for subsequent use in corpus building.

```{r}
require("stringr")

dir.create(path = "./samples/")

# Data cleaning
profanity_list <- readLines("http://www.bannedwordlist.com/lists/swearWords.txt", warn = FALSE)

## Blogs
blogs_samp_1 <- tolower(blogs_samp)
blogs_samp_2 <- str_replace_all(blogs_samp_1, "[^[:alnum:]]", " "); blogs_samp_2 <- iconv(blogs_samp_2, from="UTF-8", to="ascii", sub=""); blogs_samp_2 <- iconv(blogs_samp_2, to="ASCII//TRANSLIT")
blogs_samp_3 <- str_replace_all(blogs_samp_2, "[[:digit:]]+", " ")
blogs_samp_4 <- str_replace_all(blogs_samp_3, paste(profanity_list, collapse = "|"), replacement = "")
blogs_samp_5 <- str_replace_all(blogs_samp_4, "  ", replacement = " ")
write.table(blogs_samp_5, file = "./samples/blogs_sample.txt")

## News
news_samp_1 <- tolower(news_samp)
news_samp_2 <- str_replace_all(news_samp_1, "[^[:alnum:]]", " "); news_samp_2 <- iconv(news_samp_2, from="UTF-8", to="ascii", sub=""); news_samp_2 <- iconv(blogs_samp_2, to="ASCII//TRANSLIT")
news_samp_3 <- str_replace_all(news_samp_2, "[[:digit:]]+", " ")
news_samp_4 <- str_replace_all(news_samp_3, paste(profanity_list, collapse = "|"), replacement = "")
news_samp_5 <- str_replace_all(news_samp_4, "  ", replacement = " ")
write.table(news_samp_5, file = "./samples/news_sample.txt")

## Tweets
tweets_samp_1 <- tolower(tweets_samp)
tweets_samp_2 <- str_replace_all(tweets_samp_1, "[^[:alnum:]]", " "); tweets_samp_2 <- iconv(tweets_samp_2, from="UTF-8", to="ascii", sub=""); tweets_samp_2 <- iconv(tweets_samp_2, to="ASCII//TRANSLIT")
tweets_samp_3 <- str_replace_all(tweets_samp_2, "[[:digit:]]+", " ")
tweets_samp_4 <- str_replace_all(tweets_samp_3, paste(profanity_list, collapse = "|"), replacement = "")
tweets_samp_5 <- str_replace_all(tweets_samp_4, "  ", replacement = " ")
write.table(tweets_samp_5, file = "./samples/tweets_sample.txt")

## Clean up
rm(profanity_list)
rm(blogs_samp); rm(blogs_samp_1); rm(blogs_samp_2); rm(blogs_samp_3); rm(blogs_samp_4); rm(blogs_samp_5)
rm(news_samp); rm(news_samp_1); rm(news_samp_2); rm(news_samp_3); rm(news_samp_4); rm(news_samp_5)
rm(tweets_samp); rm(tweets_samp_1); rm(tweets_samp_2); rm(tweets_samp_3); rm(tweets_samp_4); rm(tweets_samp_5)

```

## Tokenization and n-gram construction

In the next step of data processing, we load the 3 samples into R's [stylo] (https://sites.google.com/site/computationalstylistics/stylo) package. This initial step also tokenizes the data, then allows us to build unigrams, bigrams, trigrams and 4-grams. 

```{r}
require("stylo")

# Load data into stylo corpus objects
blogs_corp <- load.corpus(files = "blogs_sample.txt", corpus.dir = "./samples")
news_corp <- load.corpus(files = "news_sample.txt", corpus.dir = "./samples")
tweets_corp <- load.corpus(files = "tweets_sample.txt", corpus.dir = "./samples")


# Tokenize data
blogs_tok <- txt.to.words(blogs_corp)
news_tok <- txt.to.words(news_corp)
tweets_tok <- txt.to.words(tweets_corp)


# Make 2-, 3- and 4-grams
blogs_tok_2 <- make.ngrams(blogs_tok, ngram.size = 2); blogs_tok_3 <- make.ngrams(blogs_tok, ngram.size = 3); blogs_tok_4 <- make.ngrams(blogs_tok, ngram.size = 4)

news_tok_2 <- make.ngrams(news_tok, ngram.size = 2); news_tok_3 <- make.ngrams(news_tok, ngram.size = 3); news_tok_4 <- make.ngrams(news_tok, ngram.size = 4)

tweets_tok_2 <- make.ngrams(tweets_tok, ngram.size = 2); tweets_tok_3 <- make.ngrams(tweets_tok, ngram.size = 3); tweets_tok_4 <- make.ngrams(tweets_tok, ngram.size = 4)

# Clean up
rm(blogs_corp); rm(news_corp); rm(tweets_corp)

```


Next, we build word lists and frequency tables. These will allow us to get an initial idea of highly frequent sequences against which we can subsequently test our text prediction models.


```{r}

# Make word lists and frequency tables

## Blogs sorted word lists
blogs_words = names(sort(table(unlist(blogs_tok)), decreasing = TRUE)); blogs_words_2 = names(sort(table(unlist(blogs_tok_2)), decreasing = TRUE)); blogs_words_3 = names(sort(table(unlist(blogs_tok_3)), decreasing = TRUE)); blogs_words_4 = names(sort(table(unlist(blogs_tok_4)), decreasing = TRUE)) 

## News sorted word lists
news_words = names(sort(table(unlist(news_tok)), decreasing = TRUE)); news_words_2 = names(sort(table(unlist(news_tok_2)), decreasing = TRUE)); news_words_3 = names(sort(table(unlist(news_tok_3)), decreasing = TRUE)); news_words_4 = names(sort(table(unlist(news_tok_4)), decreasing = TRUE))

## Tweets sorted word lists
tweets_words = names(sort(table(unlist(tweets_tok)), decreasing = TRUE)); tweets_words_2 = names(sort(table(unlist(tweets_tok_2)), decreasing = TRUE)); tweets_words_3 = names(sort(table(unlist(tweets_tok_3)), decreasing = TRUE)); tweets_words_4 = names(sort(table(unlist(tweets_tok_4)), decreasing = TRUE))


## Blogs frequency counts and tables
x_name <- "grams"; y_name <- "frequency"

blogs_freq_1 <- as.numeric(make.table.of.frequencies(blogs_tok, blogs_words, relative = FALSE))
blogs_freq_tab_1 <- data.frame(blogs_words, blogs_freq_1); names(blogs_freq_tab_1) <- c(x_name, y_name)

blogs_freq_2 <- as.numeric(make.table.of.frequencies(blogs_tok_2, blogs_words_2, relative = FALSE))
blogs_freq_tab_2 <- data.frame(blogs_words_2, blogs_freq_2); names(blogs_freq_tab_2) <- c(x_name, y_name)

blogs_freq_3 <- as.numeric(make.table.of.frequencies(blogs_tok_3, blogs_words_3, relative = FALSE))
blogs_freq_tab_3 <- data.frame(blogs_words_3, blogs_freq_3); names(blogs_freq_tab_3) <- c(x_name, y_name)

blogs_freq_4 <- as.numeric(make.table.of.frequencies(blogs_tok_4, blogs_words_4, relative = FALSE))
blogs_freq_tab_4 <- data.frame(blogs_words_4, blogs_freq_4); names(blogs_freq_tab_4) <- c(x_name, y_name)

## News frequency counts
news_freq_1 <- as.numeric(make.table.of.frequencies(news_tok, news_words, relative = FALSE))
news_freq_tab_1 <- data.frame(news_words, news_freq_1); names(news_freq_tab_1) <- c(x_name, y_name)

news_freq_2 <- as.numeric(make.table.of.frequencies(news_tok_2, news_words_2, relative = FALSE))
news_freq_tab_2 <- data.frame(news_words_2, news_freq_2); names(news_freq_tab_2) <- c(x_name, y_name)

news_freq_3 <- as.numeric(make.table.of.frequencies(news_tok_3, news_words_3, relative = FALSE))
news_freq_tab_3 <- data.frame(news_words_3, news_freq_3); names(news_freq_tab_3) <- c(x_name, y_name)

news_freq_4 <- as.numeric(make.table.of.frequencies(news_tok_4, news_words_4, relative = FALSE))
news_freq_tab_4 <- data.frame(news_words_4, news_freq_4); names(news_freq_tab_4) <- c(x_name, y_name)

## Tweets frequency counts
tweets_freq_1 <- as.numeric(make.table.of.frequencies(tweets_tok, tweets_words, relative = FALSE))
tweets_freq_tab_1 <- data.frame(tweets_words, tweets_freq_1); names(tweets_freq_tab_1) <- c(x_name, y_name)

tweets_freq_2 <- as.numeric(make.table.of.frequencies(tweets_tok_2, tweets_words_2, relative = FALSE))
tweets_freq_tab_2 <- data.frame(tweets_words_2, tweets_freq_2); names(tweets_freq_tab_2) <- c(x_name, y_name)

tweets_freq_3 <- as.numeric(make.table.of.frequencies(tweets_tok_3, tweets_words_3, relative = FALSE))
tweets_freq_tab_3 <- data.frame(tweets_words_3, tweets_freq_3); names(tweets_freq_tab_3) <- c(x_name, y_name)

tweets_freq_4 <- as.numeric(make.table.of.frequencies(tweets_tok_4, tweets_words_4, relative = FALSE))
tweets_freq_tab_4 <- data.frame(tweets_words_4, tweets_freq_4); names(tweets_freq_tab_4) <- c(x_name, y_name)



# Barplots of frequent n-grams in several corpora

## Blogs 1-grams
blogs_freq_tab_1_sub <- as.data.frame(head(blogs_freq_tab_1, n = 20)); blogs_freq_tab_1_sub$order <- factor(blogs_freq_tab_1_sub$gram, as.character(blogs_freq_tab_1_sub$gram))

blogs_freq_tab_1_sub.plot <- ggplot(blogs_freq_tab_1_sub, aes(x = blogs_freq_tab_1_sub$order, y = blogs_freq_tab_1_sub$frequency)) + 
    geom_bar(stat="identity") + coord_flip() + xlab("") + ylab("") + ggtitle("Top 20 1-grams in Blogs Corpus") +
    geom_bar(colour="green", fill="blue", width=.7, stat="identity") + 
    theme(plot.background = element_blank() ,panel.grid.major = element_blank() ,panel.grid.minor = element_blank(),     
    panel.border = element_blank())

plot(blogs_freq_tab_1_sub.plot)

## News 2-grams
news_freq_tab_2_sub <- as.data.frame(head(news_freq_tab_2, n = 20)); news_freq_tab_2_sub$order <- factor(news_freq_tab_2_sub$gram, as.character(news_freq_tab_2_sub$gram))

news_freq_tab_2_sub.plot <- ggplot(news_freq_tab_2_sub, aes(x = news_freq_tab_2_sub$order, y = news_freq_tab_2_sub$frequency)) + 
    geom_bar(stat="identity") + coord_flip() + xlab("") + ylab("") + ggtitle("Top 20 2-grams in News Corpus") +
    geom_bar(colour="pink", fill="purple", width=.7, stat="identity") + 
    theme(plot.background = element_blank() ,panel.grid.major = element_blank() ,panel.grid.minor = element_blank(),     
    panel.border = element_blank())

plot(news_freq_tab_2_sub.plot)

## Tweets 3-grams

tweets_freq_tab_3_sub <- as.data.frame(head(tweets_freq_tab_3, n = 20)); tweets_freq_tab_3_sub$order <- factor(tweets_freq_tab_3_sub$gram, as.character(tweets_freq_tab_3_sub$gram))

tweets_freq_tab_3_sub.plot <- ggplot(tweets_freq_tab_3_sub, aes(x = tweets_freq_tab_3_sub$order, y = tweets_freq_tab_3_sub$frequency)) + 
    geom_bar(stat="identity") + coord_flip() + xlab("") + ylab("") + ggtitle("Top 20 3-grams in Twitter Corpus") +
    geom_bar(colour="green", fill="yellow", width=.7, stat="identity") + 
    theme(plot.background = element_blank() ,panel.grid.major = element_blank() ,panel.grid.minor = element_blank(),     
    panel.border = element_blank())

plot(tweets_freq_tab_3_sub.plot)

## Blogs 4-grams

blogs_freq_tab_4_sub <- as.data.frame(head(blogs_freq_tab_4, n = 20)); blogs_freq_tab_4_sub$order <- factor(blogs_freq_tab_4_sub$gram, as.character(blogs_freq_tab_4_sub$gram))

blogs_freq_tab_4_sub.plot <- ggplot(blogs_freq_tab_4_sub, aes(x = blogs_freq_tab_4_sub$order, y = blogs_freq_tab_4_sub$frequency)) + 
    geom_bar(stat="identity") + coord_flip() + xlab("") + ylab("") + ggtitle("Top 20 4-grams in Blogs Corpus") +
    geom_bar(colour="orange", fill="turquoise", width=.7, stat="identity") + 
    theme(plot.background = element_blank() ,panel.grid.major = element_blank() ,panel.grid.minor = element_blank(),     
    panel.border = element_blank())

plot(blogs_freq_tab_4_sub.plot)

# Clean up
rm(x_name); rm(y_name)
rm(blogs_tok); rm(blogs_tok_2); rm(blogs_tok_3); rm(blogs_tok_4)
rm(blogs_words); rm(blogs_words_2); rm(blogs_words_3); rm(blogs_words_4)
rm(blogs_freq_1); rm(blogs_freq_2); rm(blogs_freq_3); rm(blogs_freq_4)

rm(news_tok); rm(news_tok_2); rm(news_tok_3); rm(news_tok_4)
rm(news_words); rm(news_words_2); rm(news_words_3); rm(news_words_4)
rm(news_freq_1); rm(news_freq_2); rm(news_freq_3); rm(news_freq_4)

rm(tweets_tok); rm(tweets_tok_2); rm(tweets_tok_3); rm(tweets_tok_4)
rm(tweets_words); rm(tweets_words_2); rm(tweets_words_3); rm(tweets_words_4)
rm(tweets_freq_1); rm(tweets_freq_2); rm(tweets_freq_3); rm(tweets_freq_4)

rm(blogs_freq_tab_1_sub); rm(blogs_freq_tab_1_sub.plot)
rm(news_freq_tab_2_sub); rm(news_freq_tab_2_sub.plot)
rm(tweets_freq_tab_3_sub); rm(tweets_freq_tab_3_sub.plot)
rm(blogs_freq_tab_4_sub); rm(blogs_freq_tab_4_sub.plot)
```
                                                                                                                                                                                                  

## Modeling 

For the milestone:

- mention thorny issue of contractions; try to come up with a principled way of tackling the problem.
- address goals for the eventual app and algorithm
- briefly summarize your plans for creating the prediction algorithm and Shiny app